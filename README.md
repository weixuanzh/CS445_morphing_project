# CS445_morphing_project

All methods are organized in the "Methods.py" file. Our workflow is described as follows: The image morphing procedure contains 3 main steps: key point picking, triangularization, and warping each triangular region. In our project, we experimented with different key point picking methods: manual, ADNet, and Mediapipe. Out of the three methods, ADNet and Mediapipe are neural-network-based facial landmark detection methods, where key points like mouse, nose, and eyes are identificed automatically by a convolutional neural network when face images are fed as inputs. The introduction of the two methods enables a fully automatic morphing pipeline without the need for manual point picking.

We have a box folder which stores the images pairs used for morphing and the results (https://uofi.app.box.com/folder/320630756442?s=afaabqpkwlmbfyo8vtbko6fc7pismhlz). To use the files, download the folder and put the files/folders inside it (for example, image_pairs and train.pkl) in the same directory level as the notebook "Final_Project_CS445.ipynb".

To obtain morphing results, see code in "Final_Project_CS445.ipynb". The image pairs are already included in the repository in the folder "image_pairs", and results generated by the team are stored in "outputs" folder. To change the method of morphing, simply change the variable "method" in the notebook. Possible values are "MANUAL", "ADNET", and "MP". Note that they should all be uppercase.

To run key point detection with ADNet, a checkpoint for a pretrained model should be downloaded. It can be downloaded from the link (https://drive.google.com/drive/folders/193KichBpbSG9IdgJVw-GK47hmFDcPaib). Once it is saved locally, the variable "adnet_ckpt_dir" in the notebook should be updated with where you saved the file "train.pkl".

To run key point detection with Mediapipe, python version should be lower than 3.13.

We also experimented a morphing method based on diffusion models, the repository used is DiffMorpher by Kaiwen Zhang et al. (https://github.com/Kevin-thu/DiffMorpher). Since no modification is made to their code, the source code is not included in our repository. The morphing results are saved in "outputs" folder, where the subfolders "results_pair_x" are collection of results generated from the diffusion model. Each of them includes 15 images which are intermediate frames and a "output.gif" which is an animation.

To run the DiffMorpher code, after setup according to the instructions given in DiffMorpher repository, run the command:

python main.py --image_path_0 [image_path_0] --image_path_1 [image_path_1] --prompt_0 [prompt_0]--prompt_1 [prompt_1] --output_path [output_path] --use_adain --use_reschedule --save_inter

To get results, we used empty prompts (prompt_0 and prompt_1 are just "", empty strings), and the images are taken from our image pairs.

## Attribution

This project includes code from the [ADNet repository](https://github.com/huangyangyu/ADNet) by Yangyu Huang et al., used under the MIT License.

Original work:
Huang, Yangyu, et al. "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment." *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 2021, pp. 3080–3090.

MIT License © 2021 Yangyu Huang