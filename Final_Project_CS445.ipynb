{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb69b4dd-9c43-42ec-8df0-6700338cbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "# from external.ADNet.lib.backbone import stackedHGNetV1\n",
    "from external.ADNet.lib.backbone import stackedHGNetV1\n",
    "from external.ADNet.conf.alignment import Alignment\n",
    "import torch\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc5d7d3-7539-4e7e-bd44-6463487cf705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_coordinates(h, w, landmarks):\n",
    "    x_pixel = ((landmarks[:, :, 0] + 1) / 2) * w\n",
    "    y_pixel = ((landmarks[:, :, 1] + 1) / 2) * h\n",
    "    return torch.stack((x_pixel, y_pixel), dim=2).squeeze(0).cpu().numpy()\n",
    "# function to initalize ADNet\n",
    "def initialize_net(model_path, device=torch.device(\"cuda\")):\n",
    "    config = Alignment()\n",
    "\n",
    "    net = stackedHGNetV1.StackedHGNetV1(classes_num=config.classes_num, \\\n",
    "                                        edge_info=config.edge_info, \\\n",
    "                                        nstack=config.nstack, \\\n",
    "                                        add_coord=config.add_coord, \\\n",
    "                                        pool_type=config.pool_type, \\\n",
    "                                        use_multiview=config.use_multiview)\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    net.load_state_dict(checkpoint[\"net\"])\n",
    "\n",
    "    # send to gpu, set to evaluation mode\n",
    "    net = net.float().to(device)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "# pass the image throguh ADNet to get landmarks in image coordinates\n",
    "def get_landmarks_ADNet(img, net, device=torch.device(\"cuda\")):\n",
    "    old_h, old_w = img.shape[:2]\n",
    "    # preprocess image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    # get landmarks\n",
    "    with torch.no_grad():\n",
    "        _, _, landmarks = net(img)\n",
    "        landmarks = get_actual_coordinates(old_h, old_w, landmarks)\n",
    "\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de297504-afb3-4f7d-a56d-7256fa768a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_points(img, title=\"Select points\"):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(title)\n",
    "    points = plt.ginput(n=-1, timeout=0)\n",
    "    plt.close()\n",
    "    return np.array(points)\n",
    "\n",
    "def add_boundary_points(img_shape):\n",
    "    h, w = img_shape[:2]\n",
    "    return np.array([\n",
    "        [0, 0], [w-1, 0], [w-1, h-1], [0, h-1],\n",
    "        [w//2, 0], [w-1, h//2], [w//2, h-1], [0, h//2]\n",
    "    ])\n",
    "\n",
    "def morph_triangle(img1, img2, img, t1, t2, t, alpha):\n",
    "    # Compute affine transform matrices\n",
    "    warp_mat1 = cv2.getAffineTransform(np.float32(t1), np.float32(t))\n",
    "    warp_mat2 = cv2.getAffineTransform(np.float32(t2), np.float32(t))\n",
    "\n",
    "    # Warp triangles\n",
    "    warped_img1 = cv2.warpAffine(img1, warp_mat1, (img.shape[1], img.shape[0]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "    warped_img2 = cv2.warpAffine(img2, warp_mat2, (img.shape[1], img.shape[0]), None, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    # Mask for the triangle\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)\n",
    "    cv2.fillConvexPoly(mask, np.int32(t), 1.0, 16, 0)\n",
    "\n",
    "    # Blend the triangles\n",
    "    img += mask * ((1.0 - alpha) * warped_img1 + alpha * warped_img2)\n",
    "\n",
    "def morph_images(img1, img2, points1, points2, tri, alpha):\n",
    "    h, w, c = img1.shape  \n",
    "    morphed_img = np.zeros((h, w, c), dtype=np.float32)\n",
    "\n",
    "    points = (1 - alpha) * points1 + alpha * points2\n",
    "\n",
    "    for tri_indices in tri.simplices:\n",
    "        x1 = points1[tri_indices]\n",
    "        x2 = points2[tri_indices]\n",
    "        x = points[tri_indices]\n",
    "\n",
    "        # For each channel separately\n",
    "        for ch in range(c):\n",
    "            morph_triangle(img1[:,:,ch], img2[:,:,ch], morphed_img[:,:,ch], x1, x2, x, alpha)\n",
    "\n",
    "    return np.clip(morphed_img, 0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be1981f-1a60-416d-8c4b-58ea4b705db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_symmetry_accuracy_test(): #Used to measure the symmetry or bias in the interpolation done by the morph images function\n",
    "    #If both images are the same for A->B and B->A at 0.5, that means there is no inconsistencies and the warping is accurate\n",
    "\n",
    "    folder_path = './image_set'\n",
    "    image_names = [name for name in os.listdir(folder_path) if name.lower().endswith('.jpg')]\n",
    "    loaded_images = [ cv2.resize((cv2.cvtColor(cv2.imread(folder_path + '/' + name),cv2.COLOR_BGR2RGB)), (128,128)) for name in image_names]\n",
    "    #loaded_images = [ cv2.imread(folder_path + '/' + name) for name in image_names]\n",
    "\n",
    "    face_mesh_tools = mp.solutions.face_mesh\n",
    "    face_mesh = face_mesh_tools.FaceMesh(static_image_mode=True,max_num_faces=1,refine_landmarks=True,min_detection_confidence=0.5)\n",
    "    important_landmarks = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323,361,288, 397, 365, 379, 378, 400, 152, 148, 176, 136, 172,58, 132, 234, 127, 162, 21, 54, 103, 67, 109, 33, 133, 159, 145, 160, 144, 153, 154, 155,362, 263, 386, 374, 387, 373, 380, 381, 382,1, 2, 98, 327, 94, 331, 168, 197, 195, 5, 4,61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 78, 95, 88, 178]\n",
    "    facial_points = []\n",
    "    no_face = []\n",
    "    count_for_removal = 0\n",
    "    \n",
    "    for l_image in loaded_images:\n",
    "        landmarks = face_mesh.process(l_image)\n",
    "        height,width,_ = l_image.shape\n",
    "    \n",
    "        if landmarks.multi_face_landmarks: #if any face at all is detected, we select the first one\n",
    "            facial_landmarks = landmarks.multi_face_landmarks[0] #they are spit out as a normalized value so we have to reintroduce them\n",
    "            important_points_per_image = []\n",
    "            for il in important_landmarks:\n",
    "                x,y =  int(facial_landmarks.landmark[il].x * width), int(facial_landmarks.landmark[il].y * height)\n",
    "                important_points_per_image.append((x,y))\n",
    "            facial_points.append(important_points_per_image)\n",
    "        else:\n",
    "            no_face.append(count_for_removal)\n",
    "        count_for_removal += 1\n",
    "\n",
    "    for entry in sorted(no_face, reverse=True): #get rid of images that contain no detectable face\n",
    "        loaded_images.pop(entry)\n",
    "        no_face.remove(entry)\n",
    "\n",
    "    error = 0\n",
    "    \n",
    "    for j in range(len(loaded_images)):\n",
    "        first_image = loaded_images[j]\n",
    "        first_points1 = facial_points[j]\n",
    "        boundary_points = add_boundary_points(first_image.shape)\n",
    "        first_points = np.vstack([first_points1,boundary_points])\n",
    "        \n",
    "        for i in range(len(loaded_images)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            curr_image = loaded_images[i]\n",
    "            curr_points = np.vstack([facial_points[i],boundary_points])\n",
    "            avg_points = (first_points + curr_points) / 2\n",
    "            triangular = Delaunay(avg_points)\n",
    "            \n",
    "            pt1 = morph_images(first_image, curr_image, first_points, curr_points, triangular, 0.5)\n",
    "            pt2 = morph_images(curr_image, first_image, curr_points, first_points, triangular, 0.5)\n",
    "            error += np.mean(np.abs(pt1.astype(np.float32) - pt2.astype(np.float32)))\n",
    "            \n",
    "    total_comparisons = len(loaded_images) * (len(loaded_images) - 1)\n",
    "    return (100 - error / total_comparisons)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7a276ef-e042-415a-b9f3-53456d6f25d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_closest_points(available_points,point,n=10):\n",
    "    distance = np.sqrt(np.sum((available_points - point) ** 2, axis=1))\n",
    "    toreturn = available_points[distance <= n]\n",
    "    return toreturn\n",
    "    \n",
    "def get_canny_points(img1, interval=50): #DOESN'T WORK WITH THE IMAGE MORPHING AT ALL, REMOVED FROM THE WRAPPER\n",
    "    img = img1.copy()\n",
    "    if img.dtype != \"uint8\":\n",
    "        img = (255 * img).clip(0, 255).astype(\"uint8\")\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image failed to load. Check the path or file.\")\n",
    "\n",
    "    edges = cv2.Canny(img,100,200)\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    selected_points = select_points(img, title=\"Select primary points\")\n",
    "    flattened = np.vstack([c.reshape(-1, 2) for c in contours])\n",
    "\n",
    "    sampled_points = np.empty((0, 2))\n",
    "    for pt in selected_points:\n",
    "        returnStart = top_closest_points(flattened,pt)\n",
    "        sampled_points = np.vstack((sampled_points, returnStart))\n",
    "\n",
    "    more_sampled = flattened[::interval]\n",
    "\n",
    "    final_points = np.vstack((selected_points,sampled_points,more_sampled))\n",
    "\n",
    "    return np.array([tuple(point) for point in final_points])\n",
    "\n",
    "def get_mp_points(img):\n",
    "    face_mesh_tools = mp.solutions.face_mesh\n",
    "    face_mesh = face_mesh_tools.FaceMesh(static_image_mode=True,max_num_faces=1,refine_landmarks=True,min_detection_confidence=0.5)\n",
    "    important_landmarks = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323,361,288, 397, 365, 379, 378, 400, 152, 148, 176, 136, 172,58, 132, 234, 127, 162, 21, 54, 103, 67, 109, 33, 133, 159, 145, 160, 144, 153, 154, 155,362, 263, 386, 374, 387, 373, 380, 381, 382,1, 2, 98, 327, 94, 331, 168, 197, 195, 5, 4,61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 78, 95, 88, 178]\n",
    "    facial_points = []\n",
    "    landmarks = face_mesh.process(img)\n",
    "    height,width,_ = img.shape\n",
    "\n",
    "    if landmarks.multi_face_landmarks: #if any face at all is detected, we select the first one\n",
    "        facial_landmarks = landmarks.multi_face_landmarks[0] #they are spit out as a normalized value so we have to reintroduce them\n",
    "        for il in important_landmarks:\n",
    "            x,y =  int(facial_landmarks.landmark[il].x * width), int(facial_landmarks.landmark[il].y * height)\n",
    "            facial_points.append((x,y))\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    return np.array(facial_points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "399b7a39-2289-46ae-ba73-13f8224f4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_Morphing_Video(imgA,imgB,videoname='morph_video.mp4',point_selection='MANUAL'):\n",
    "    possible_point_selections = ['MANUAL', 'MP', 'ADNET']\n",
    "    \n",
    "    if point_selection not in possible_point_selections:\n",
    "        print(f\"Invalid point_selection method. Choose from: {possible_point_selections}\")\n",
    "        return\n",
    "        \n",
    "    if len(imgA.shape) != len(imgB.shape):\n",
    "        print(\"Both Images Aren't Using the Same Color Channels\")\n",
    "        return\n",
    "\n",
    "    imgB = cv2.resize(imgB, (imgA.shape[1], imgA.shape[0]))\n",
    "    \n",
    "    points1 = []\n",
    "    points2 = []\n",
    "    tri = []\n",
    "    boundary_points = add_boundary_points(imgA.shape)\n",
    "    h,w,_ = imgA.shape\n",
    "    \n",
    "    if point_selection == 'MANUAL':\n",
    "        points1 = np.vstack([select_points(imgA),boundary_points])\n",
    "        points2 = np.vstack([select_points(imgB),boundary_points])\n",
    "        \n",
    "    elif point_selection == 'MP':\n",
    "        mp_points_1 = get_mp_points(imgA)\n",
    "        if mp_points_1 is None:\n",
    "            print(\"imgA Has No Detectable Facial Structure, Please Use ADNET or MANUAL\")\n",
    "            return None\n",
    "        mp_points_2 = get_mp_points(imgB)\n",
    "        if mp_points_2 is None:\n",
    "            print(\"imgB Has No Detectable Facial Structure, Please Use ADNET or MANUAL\")\n",
    "            return None\n",
    "        points1 = np.vstack([mp_points_1,boundary_points])\n",
    "        points2 = np.vstack([mp_points_2,boundary_points])\n",
    "        \n",
    "    elif point_selection == 'ADNET':\n",
    "        #TODO?\n",
    "        print(\"Not Sure How to Get the ADNET DATA In?\")\n",
    "\n",
    "    if points1.shape[0] != points2.shape[0]:\n",
    "        diff = abs(points1.shape[0] - points2.shape[0])\n",
    "        if points1.shape[0] < points2.shape[0]:\n",
    "            extra = points2[-diff:]\n",
    "            points1 = np.vstack([points1, extra])\n",
    "        else:\n",
    "            extra = points1[-diff:]\n",
    "            points2 = np.vstack([points2, extra])\n",
    "\n",
    "    avg_points = (points1 + points2) / 2\n",
    "    tri = Delaunay(avg_points)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_out = cv2.VideoWriter(videoname, fourcc, 15.0, (w, h), isColor=True)\n",
    "\n",
    "    num_frames = 100  # Forward frames\n",
    "    frames = []\n",
    "    \n",
    "    for i, alpha in enumerate(np.linspace(0, 1, num_frames)):\n",
    "        print(f\"Generating forward frame {i+1}/{num_frames}...\")\n",
    "        frame = morph_images(imgA, imgB, points1, points2, tri, alpha)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    for frame in frames:\n",
    "        video_out.write(frame)\n",
    "    \n",
    "    # Write backward frames (skip last frame to avoid repeated frame)\n",
    "    for frame in frames[-2::-1]:  # Start second to last frame, reverse\n",
    "        video_out.write(frame)\n",
    "    video_out.release()\n",
    "    print(f\"video saved as {videoname} \")\n",
    "\n",
    "    cap = cv2.VideoCapture(videoname)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "    \n",
    "    print(\"Playing video... Press Enter in the terminal to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        if not ret:\n",
    "            # If video ended, restart from beginning\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "    \n",
    "        cv2.imshow('Morph Video Preview', frame)\n",
    "    \n",
    "        # Check for keypress every 30ms\n",
    "        if cv2.waitKey(70) == 13:  # 13 is Enter key\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fbb5acd-d368-4613-a8b9-3c0469c057bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_Morphing_Image(imgA,imgB,alpha=0.8,point_selection='MANUAL'):\n",
    "    possible_point_selections = ['MANUAL', 'MP', 'ADNET']\n",
    "    \n",
    "    if point_selection not in possible_point_selections:\n",
    "        print(f\"Invalid point_selection method. Choose from: {possible_point_selections}\")\n",
    "        return\n",
    "        \n",
    "    if len(imgA.shape) != len(imgB.shape):\n",
    "        print(\"Both Images Aren't Using the Same Color Channels\")\n",
    "        return\n",
    "\n",
    "    imgB = cv2.resize(imgB, (imgA.shape[1], imgA.shape[0]))\n",
    "    \n",
    "    points1 = []\n",
    "    points2 = []\n",
    "    tri = []\n",
    "    boundary_points = add_boundary_points(imgA.shape)\n",
    "    \n",
    "    if point_selection == 'MANUAL':\n",
    "        points1 = np.vstack([select_points(imgA),boundary_points])\n",
    "        points2 = np.vstack([select_points(imgB),boundary_points])\n",
    "        \n",
    "    elif point_selection == 'MP':\n",
    "        mp_points_1 = get_mp_points(imgA)\n",
    "        if mp_points_1 is None:\n",
    "            print(\"imgA Has No Detectable Facial Structure, Please Use ADNET or MANUAL\")\n",
    "            return None\n",
    "        mp_points_2 = get_mp_points(imgB)\n",
    "        if mp_points_2 is None:\n",
    "            print(\"imgB Has No Detectable Facial Structure, Please Use ADNET or MANUAL\")\n",
    "            return None\n",
    "        points1 = np.vstack([mp_points_1,boundary_points])\n",
    "        points2 = np.vstack([mp_points_2,boundary_points])\n",
    "        \n",
    "    elif point_selection == 'ADNET':\n",
    "        #TODO?\n",
    "        print(\"Not Sure How to Get the ADNET DATA In?\")\n",
    "\n",
    "    if points1.shape[0] != points2.shape[0]:\n",
    "        diff = abs(points1.shape[0] - points2.shape[0])\n",
    "        if points1.shape[0] < points2.shape[0]:\n",
    "            extra = points2[-diff:]\n",
    "            points1 = np.vstack([points1, extra])\n",
    "        else:\n",
    "            extra = points1[-diff:]\n",
    "            points2 = np.vstack([points2, extra])\n",
    "        \n",
    "        \n",
    "    avg_points = (points1 + points2) / 2\n",
    "    tri = Delaunay(avg_points)\n",
    "\n",
    "    return morph_images(imgA, imgB, points1, points2, tri, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2546242c-709a-4cb8-9b04-4b3567b5872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"face_img.jpg\")\n",
    "img2 = cv2.imread(\"./image_set/10_0_1_20170110220507258.jpg\")\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "094c540e-c2e9-418a-ab77-ebfadc8c12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_image = Image_Morphing_Image(img1,img2,alpha=0.5,point_selection='MP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4c9e2-bbac-43b7-8ed6-5c432494547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_video = Image_Morphing_Video(img1,img2,point_selection='MP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0973ea9-ac7e-42ff-ad8f-98fd7c99d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Image Window', returned_image)\n",
    "cv2.waitKey(0)  # Waits until any key is pressed\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e54d72-2851-451c-b5d0-f0d05551e116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
